Practical Machine Learning Project

Predict Qualitative Activity Recognition of Weight Lifting Exercises

Created July-26-2014

Analysis the data

library(ggplot2)
## Warning: package 'ggplot2' was built under R version 2.15.2
library(AppliedPredictiveModeling)
## Warning: package 'AppliedPredictiveModeling' was built under R version
## 2.15.3
Load the training and testing Data

# switch all blank values in each column to NAs
training <- read.csv(file = "/Users/gaoyang/desktop/pml-training.csv", na.string = c("", 
    "NA"))
testing <- read.csv(file = "/Users/gaoyang/desktop/pml-testing.csv", na.string = c("", 
    "NA"))
Summary Datasets

dim(training)
## [1] 19622   160
dim(testing)
## [1]  20 160
summary(training$classe)
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607
Unilateral Dumbbell Biceps Curl with 5-levels in training dataset: There is 5580 samples in class A,3797 samples in class B,3422 samples in class c, 3216 samples in class D,and 3607 samples in last level.

qplot(classe, data = training, geom = "histogram", binwidth = 0.1)
plot of chunk unnamed-chunk-4

Clean Data

# find how many observations are left in each variable

columnNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
columnNAs(training)
##   [1]     0     0     0     0     0     0     0     0     0     0     0
##  [12] 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216
##  [23] 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216
##  [34] 19216 19216 19216     0     0     0     0     0     0     0     0
##  [45]     0     0     0     0     0 19216 19216 19216 19216 19216 19216
##  [56] 19216 19216 19216 19216     0     0     0     0     0     0     0
##  [67]     0     0 19216 19216 19216 19216 19216 19216 19216 19216 19216
##  [78] 19216 19216 19216 19216 19216 19216     0     0     0 19216 19216
##  [89] 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216 19216
## [100] 19216 19216     0 19216 19216 19216 19216 19216 19216 19216 19216
## [111] 19216 19216     0     0     0     0     0     0     0     0     0
## [122]     0     0     0 19216 19216 19216 19216 19216 19216 19216 19216
## [133] 19216 19216 19216 19216 19216 19216 19216     0 19216 19216 19216
## [144] 19216 19216 19216 19216 19216 19216 19216     0     0     0     0
## [155]     0     0     0     0     0     0

# find columns are not all NAs
which(columnNAs(training) != 19216)
##  [1]   1   2   3   4   5   6   7   8   9  10  11  37  38  39  40  41  42
## [18]  43  44  45  46  47  48  49  60  61  62  63  64  65  66  67  68  84
## [35]  85  86 102 113 114 115 116 117 118 119 120 121 122 123 124 140 151
## [52] 152 153 154 155 156 157 158 159 160

# Remove allNAs and blank values from training dataset
train1 <- training[, which(columnNAs(training) != 19216)]
names(train1)
##  [1] "X"                    "user_name"            "raw_timestamp_part_1"
##  [4] "raw_timestamp_part_2" "cvtd_timestamp"       "new_window"          
##  [7] "num_window"           "roll_belt"            "pitch_belt"          
## [10] "yaw_belt"             "total_accel_belt"     "gyros_belt_x"        
## [13] "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"        
## [16] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"       
## [19] "magnet_belt_y"        "magnet_belt_z"        "roll_arm"            
## [22] "pitch_arm"            "yaw_arm"              "total_accel_arm"     
## [25] "gyros_arm_x"          "gyros_arm_y"          "gyros_arm_z"         
## [28] "accel_arm_x"          "accel_arm_y"          "accel_arm_z"         
## [31] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"        
## [34] "roll_dumbbell"        "pitch_dumbbell"       "yaw_dumbbell"        
## [37] "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"    
## [40] "gyros_dumbbell_z"     "accel_dumbbell_x"     "accel_dumbbell_y"    
## [43] "accel_dumbbell_z"     "magnet_dumbbell_x"    "magnet_dumbbell_y"   
## [46] "magnet_dumbbell_z"    "roll_forearm"         "pitch_forearm"       
## [49] "yaw_forearm"          "total_accel_forearm"  "gyros_forearm_x"     
## [52] "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"     
## [55] "accel_forearm_y"      "accel_forearm_z"      "magnet_forearm_x"    
## [58] "magnet_forearm_y"     "magnet_forearm_z"     "classe"
dim(train1)
## [1] 19622    60
Select Predictors in Training dataset


trainHAR <- train1[c(8:60)]
Based on health description in the reference,arms sensors,belt sensors,forarm sensors,and dumbbell sensors are the main four factors to explore 5 level activity recognition.After removing missing values,I select only 52 variables to treat as predictors in the final model,including roll_belt,pitch_belt,yaw_belt,roll_arm,pitch_arm,yaw_arm,roll_dumbbell,pitch_dumbbell,yaw_dumbbell,roll_forearm,pitch_forearm,and yaw_forearm.

Correlation Matrix with Belt Sensors Orientation

library(corrplot)
## Warning: package 'corrplot' was built under R version 2.15.3
var_m <- c("roll_belt", "pitch_belt", "yaw_belt")
Mat <- trainHAR[var_m]
M <- cor(Mat)
corrplot(M, method = "number")
plot of chunk unnamed-chunk-7

According to the result of maxtrix correlation,they have highly statistical correlation between yaw_belt and roll_belte,and pitch_belt.Also there is negative statistical correlation between pitch_belt and roll_belt observations.

Statistical Analysis Using PCA

In the model,classe(categorical variable) treated as an outcome variable.Because each predictors are high correlated,neither logistic regression model nor Linear Discriminant Analysis(LDA) model works well.

Option 1:K-Nearest Neighbor


attach(trainHAR)
library(caret)
## Warning: package 'caret' was built under R version 2.15.3
## Loading required package: lattice

Fit1 <- train(classe ~ ., trainHAR, method = "knn", preProcess = c("pca"), trControl = trainControl(method = "cv"))
## Warning: package 'e1071' was built under R version 2.15.3
print(Fit1)
## k-Nearest Neighbors 
## 
## 19622 samples
##    52 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## Pre-processing: principal component signal extraction, scaled, centered 
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 17659, 17658, 17661, 17659, 17660, 17660, ... 
## 
## Resampling results across tuning parameters:
## 
##   k  Accuracy  Kappa  Accuracy SD  Kappa SD
##   5  1         1      0.005        0.006   
##   7  1         1      0.005        0.006   
##   9  1         0.9    0.005        0.007   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.
I use Principle Factor Analysis with K-Nearest Neighbors(K-NN) Algorithm to reduce dimensions.In order to save more information which comes from predictors,PCA is the one of best method to explore the data.The accuracy in the model with KNN method is 97.00%(when k equals to 5).

Option 2: Random Forest

library(randomForest)
## Warning: package 'randomForest' was built under R version 2.15.1
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
Fit2 <- train(classe ~ ., trainHAR, method = "rf", preProcess = c("pca"), trControl = trainControl(method = "cv"))
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
print(Fit2)
## Random Forest 
## 
## 19622 samples
##    52 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## Pre-processing: principal component signal extraction, scaled, centered 
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 17660, 17660, 17660, 17659, 17660, 17660, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##   2     1         1      0.005        0.006   
##   30    1         1      0.003        0.004   
##   50    1         1      0.003        0.004   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.
print(Fit2$finalModel)
## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 1.66%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 5553    6   12    6    3    0.004839
## B   53 3711   28    1    4    0.022649
## C    6   33 3356   24    3    0.019287
## D    3    2   97 3109    5    0.033271
## E    0    9   17   13 3568    0.010812
I use 10-fold Cross Validation to prevent the model overfitting.Also,I try to use Random Forest algorithm in PCA model.The accuracy in the model with random forest method is 98.20%.The accuracy is a little bit higher than that in model with K-Nearest Neighbors.

To predict 20 cases with each problem id further,I use PCA model with random forest.

Prepare for Test data

columnNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
columnNAs(testing)
##   [1]  0  0  0  0  0  0  0  0  0  0  0 20 20 20 20 20 20 20 20 20 20 20 20
##  [24] 20 20 20 20 20 20 20 20 20 20 20 20 20  0  0  0  0  0  0  0  0  0  0
##  [47]  0  0  0 20 20 20 20 20 20 20 20 20 20  0  0  0  0  0  0  0  0  0 20
##  [70] 20 20 20 20 20 20 20 20 20 20 20 20 20 20  0  0  0 20 20 20 20 20 20
##  [93] 20 20 20 20 20 20 20 20 20  0 20 20 20 20 20 20 20 20 20 20  0  0  0
## [116]  0  0  0  0  0  0  0  0  0 20 20 20 20 20 20 20 20 20 20 20 20 20 20
## [139] 20  0 20 20 20 20 20 20 20 20 20 20  0  0  0  0  0  0  0  0  0  0

# find columns are not all NAs
which(columnNAs(testing) != 20)
##  [1]   1   2   3   4   5   6   7   8   9  10  11  37  38  39  40  41  42
## [18]  43  44  45  46  47  48  49  60  61  62  63  64  65  66  67  68  84
## [35]  85  86 102 113 114 115 116 117 118 119 120 121 122 123 124 140 151
## [52] 152 153 154 155 156 157 158 159 160

# Remove allNAs and blank values from testing dataset
test1 <- testing[, which(columnNAs(testing) != 20)]
names(test1)
##  [1] "X"                    "user_name"            "raw_timestamp_part_1"
##  [4] "raw_timestamp_part_2" "cvtd_timestamp"       "new_window"          
##  [7] "num_window"           "roll_belt"            "pitch_belt"          
## [10] "yaw_belt"             "total_accel_belt"     "gyros_belt_x"        
## [13] "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"        
## [16] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"       
## [19] "magnet_belt_y"        "magnet_belt_z"        "roll_arm"            
## [22] "pitch_arm"            "yaw_arm"              "total_accel_arm"     
## [25] "gyros_arm_x"          "gyros_arm_y"          "gyros_arm_z"         
## [28] "accel_arm_x"          "accel_arm_y"          "accel_arm_z"         
## [31] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"        
## [34] "roll_dumbbell"        "pitch_dumbbell"       "yaw_dumbbell"        
## [37] "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"    
## [40] "gyros_dumbbell_z"     "accel_dumbbell_x"     "accel_dumbbell_y"    
## [43] "accel_dumbbell_z"     "magnet_dumbbell_x"    "magnet_dumbbell_y"   
## [46] "magnet_dumbbell_z"    "roll_forearm"         "pitch_forearm"       
## [49] "yaw_forearm"          "total_accel_forearm"  "gyros_forearm_x"     
## [52] "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"     
## [55] "accel_forearm_y"      "accel_forearm_z"      "magnet_forearm_x"    
## [58] "magnet_forearm_y"     "magnet_forearm_z"     "problem_id"
The 20 test cases includes problem_id variable.

Predict 20 Cases with each Problem_ID in Test Data

test2 <- test1[c(8:59)]
pred <- predict(Fit2, test2)
print(pred)
##  [1] B A A A A E D B A A B C B A E E A B B B
## Levels: A B C D E
Summary

The error rate of cross validation in random forest is 1.80% based on training dataset(whole data) in validation part.The out of bag estimate error rate is 1.59%.

References

[1]Human Activity Recognition Description:http://groupware.les.inf.puc-rio.br/har

[2]Practical Machine Learning Project Description:https://class.coursera.org/predmachlearn-003/human_grading/view/courses/972148/assessments/4/submissions
